{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz956M7fZ02a",
        "outputId": "77114394-3e35-4936-c230-77d543866f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.11/dist-packages (1.24.4)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
            "Requirement already satisfied: gensim==4.3.1 in /usr/local/lib/python3.11/dist-packages (4.3.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.1) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.1) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.24.4 scipy==1.10.1 gensim==4.3.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "eeJqebNbZ18A"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkm7xUMiaP8-",
        "outputId": "f31afa83-93bc-494d-9a1b-0be2235791ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(\"/content/News_Category_Dataset_v3.json\", lines=True)\n",
        "df.to_csv(\"/content/News_Category_Dataset_v3.csv\", index=False)\n",
        "data = pd.read_csv(\"/content/News_Category_Dataset_v3.csv\")"
      ],
      "metadata": {
        "id": "06CU18O8aU6N"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = data['headline']\n",
        "\n",
        "clean_sentences = sentences.dropna().reset_index(drop=True)\n",
        "\n",
        "tokenized_sentences = [word_tokenize(str(sentence)) for sentence in clean_sentences]"
      ],
      "metadata": {
        "id": "8bNFbrI2ccMD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_sentences = []\n",
        "for sentence in tokenized_sentences:\n",
        "  lowered = [word.lower() for word in sentence]\n",
        "  tagged = pos_tag(lowered)\n",
        "  lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
        "  lemmatized_sentences.append(lemmatized)"
      ],
      "metadata": {
        "id": "1_jQFB5KdEF4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_sentences = []\n",
        "for sentence in lemmatized_sentences:\n",
        "    filtered = [word for word in sentence if word not in stop_words]\n",
        "    filtered_sentences.append(filtered)"
      ],
      "metadata": {
        "id": "ucvc9tMTdR27"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46x1xrMDdeNi",
        "outputId": "4d3af008-332a-4b0b-efea-90006572bfc0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentences_cleaned = []\n",
        "vectorized_sentences = []\n",
        "\n",
        "for sentence in filtered_sentences:\n",
        "    sentence_vectors = [wv[word] for word in sentence if word in wv]\n",
        "    if sentence_vectors:  # for only sentences that dont have NA\n",
        "        avg_vector = np.mean(sentence_vectors, axis=0)\n",
        "        vectorized_sentences.append(avg_vector)\n",
        "        filtered_sentences_cleaned.append(sentence)"
      ],
      "metadata": {
        "id": "t2Qb3LGadhUZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_vector(sentence, wv):\n",
        "\n",
        "    sentence_vectors = [wv[word] for word in sentence if word in wv]\n",
        "    if sentence_vectors:\n",
        "        return np.mean(sentence_vectors, axis=0).reshape(1, -1)\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "QFfs7kNNdlgn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_top_k_similar(input_sentence, wv, vectorized_sentences, filtered_sentences, k=5):\n",
        "    vec = get_sentence_vector(input_sentence, wv)\n",
        "    if vec is None:\n",
        "        return\n",
        "\n",
        "    similarities = cosine_similarity(vec, np.array(vectorized_sentences))[0]\n",
        "    top_k_idx = similarities.argsort()[::-1][:k]\n",
        "\n",
        "    return [(filtered_sentences[i], similarities[i]) for i in top_k_idx]"
      ],
      "metadata": {
        "id": "DrS1SLKKdpPC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = [\"president\", \"got\", \"no\", \"money\"]\n",
        "top_matches = find_top_k_similar(input_sentence, wv, vectorized_sentences, filtered_sentences)\n",
        "\n",
        "for i, (sentence, score) in enumerate(top_matches, 1):\n",
        "    print(f\"{i}. {' '.join(sentence)} (Similarity: {score:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvRfbrHGdsXF",
        "outputId": "6a4d4a15-2564-4c87-d4f8-ed439e207283"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. guy 's running president want give 'free ' money (Similarity: 0.7809)\n",
            "2. copycat chick-fil-a sandwich recipe ( hungry sunday ) (Similarity: 0.6981)\n",
            "3. clothe organization : family 's closet say ( photo ) (Similarity: 0.6963)\n",
            "4. 'la la land ' win bafta 's top prize , continue hot streak road oscar (Similarity: 0.6809)\n",
            "5. guy 's get 2 word president , 's put d.c . (Similarity: 0.6600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zqpXIxcjdvrh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}